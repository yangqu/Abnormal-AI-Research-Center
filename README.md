# ðŸš€ Abnormal AI Research Center

> *Where the weird, the wonderful, and the slightly unhinged AI research lives.*

![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Experimental-purple?style=for-the-badge)
![AI Safety](https://img.shields.io/badge/AI%20Safety-Required-red?style=for-the-badge)

---

## ðŸŽ¯ What is this?

Welcome to the **Abnormal AI Research Center** â€” a playground for exploring the fringes of artificial intelligence. 

We're interested in questions that make other researchers raise an eyebrow:

- ðŸ§  What happens when AI gets... creative?
- ðŸŽ­ Can we teach machines to have personality disorders (safely, in simulation)?
- ðŸ”® What does "abnormal" even mean when the baseline is already alien?

This is where we push boundaries, break assumptions, and occasionally write code that makes our laptops emit concerned whirring sounds.

---

## ðŸŽª Core Research Areas

| Area | Description | Risk Level |
|------|-------------|------------|
| ðŸ¤ª Emergent Behaviors | When LLMs do things we didn't expect (and can't explain) | ðŸŸ¡ Monitor |
| ðŸŽ­ Role-Playing Agents | AI personas with complex psychology | ðŸŸ  Caution |
| ðŸŒ€ Interpretability | Peering into the void (and the void peering back) | ðŸŸ¢ Safe |
| âš¡ Alignment Studies | Keeping the weird stuff friendly | ðŸ”´ Critical |

---

## ðŸ› ï¸ Quick Start

```bash
# Clone this chaos
git clone https://github.com/yangqu/Abnormal-AI-Research-Center.git

# Enter the matrix
cd Abnormal-AI-Research-Center

# Install dependencies (and your sanity)
pip install -r requirements.txt

# Run something weird
python experiments/run_the_thing.py
```

---

## ðŸ“‚ Project Structure

```
Abnormal-AI-Research-Center/
â”œâ”€â”€ ðŸ“ experiments/         # Chaos labs
â”œâ”€â”€ ðŸ“ src/                  # Core code (sort of)
â”œâ”€â”€ ðŸ“ docs/                 # For when we actually document things
â”œâ”€â”€ ðŸ“ papers/               # Preprints we couldn't get published
â”œâ”€â”€ ðŸ“ tools/                # Useful stuff (maybe)
â”œâ”€â”€ ðŸ“ playground/           # Messy sandbox for ideas
â”œâ”€â”€ README.md                # You are here
â””â”€â”€ CONTRIBUTING.md          # How to contribute (or break things)
```

---

## ðŸ¤ Contributing

Found a bug? Created something cool? Broke something spectacular?

**We want to hear about it.**

1. Fork this repo (respect the timeline)
2. Create a branch: `git checkout -b feature/my-weird-idea`
3. Commit: `git commit -m "Add something questionable"`
4. Push: `git push origin feature/my-weird-idea`
5. Open a PR and tell us a story

*Note: All contributions must pass the "Would this make a researcher at DeepMind raise an eyebrow?" test.*

---

## âš ï¸ Safety & Ethics

This is research. That means:

- ðŸ”’ **Don't** put dangerous things in production
- ðŸ§  **Think** about consequences (actual thinking, not the AI kind)
- ðŸ¤ **Share** findings responsibly
- ðŸš« **Never** release anything that could cause genuine harlÊ
We're here to understand AI, not to summon it.

---

## ðŸ“œ License

MIT License â€” because open research should be open.

---

## ðŸ“¬ Contact

- **GitHub Issues**: For bugs and feature requests
- **Discussions**: For... discussions
- **Email**: (Coming eventually)

---

## ðŸŒŸ Acknowledgments

Thanks to everyone who's ever looked at an AI doing something unexpected and said "That's weird, but what if we push it further?"

---

> *"The best AI research happens at 2 AM when you realize you've created something that technically works but you have absolutely no idea why."* â€” Some researcher, probably

---

**Built with âš¡, â˜•, and a concerning amount of curiosity**
